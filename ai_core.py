import google.generativeai as genai
import streamlit as st
import time
from google.api_core.exceptions import ResourceExhausted, ServiceUnavailable, InternalServerError, InvalidArgument

class AI_Core:
    def __init__(self):
        self.api_ready = False
        try:
            # Kiá»ƒm tra key tá»“n táº¡i trÆ°á»›c khi láº¥y
            if "api_keys" in st.secrets and "gemini_api_key" in st.secrets["api_keys"]:
                api_key = st.secrets["api_keys"]["gemini_api_key"]
                genai.configure(api_key=api_key)
                self.api_ready = True
            else:
                st.error("âš ï¸ ChÆ°a cáº¥u hÃ¬nh API Key trong secrets.toml")
                return

            # Cáº¥u hÃ¬nh Safety (Cháº·n ná»™i dung Ä‘á»™c háº¡i)
            self.safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]
            
            # Cáº¥u hÃ¬nh Generation Config (Tá»‘i Æ°u cho 3.0 Pro)
            self.gen_config = genai.GenerationConfig(
                temperature=0.8,
                max_output_tokens=32768,  # Output dÃ i thoáº£i mÃ¡i
                top_p=0.95,
                top_k=40
            )

        except Exception as e:
            st.error(f"âŒ Lá»—i khá»Ÿi táº¡o AI Core: {e}")

    def _get_model(self, model_name, system_instr=None):
        """HÃ m helper Ä‘á»ƒ khá»Ÿi táº¡o model Ä‘Ãºng phiÃªn báº£n"""
        # âœ… DANH SÃCH MODEL Má»šI NHáº¤T (Cáº­p nháº­t 2025)
        valid_names = {
            "flash": "gemini-2.5-flash",         # Nhanh, ráº»
            "pro": "gemini-2.5-pro",             # ThÃ´ng minh nháº¥t (DÃ¹ng cho tranh biá»‡n)
            "exp": "gemini-2.5-flash-exp"        # Báº£n thá»­ nghiá»‡m
        }
        
        # Máº·c Ä‘á»‹nh fallback vá» 2.5 Flash náº¿u tÃªn sai
        target_name = valid_names.get(model_name, "gemini-2.5-flash")
        
        try:
            return genai.GenerativeModel(
                model_name=target_name,
                safety_settings=self.safety_settings,
                generation_config=self.gen_config,
                system_instruction=system_instr
            )
        except Exception as e:
            # st.warning(f"âš ï¸ KhÃ´ng thá»ƒ khá»Ÿi táº¡o model {target_name}: {e}")
            return None

    def generate(self, prompt, model_type="flash", system_instruction=None):
        """
        HÃ m gá»i AI chÃ­nh: Tá»± Ä‘á»™ng chuyá»ƒn model náº¿u lá»—i (Fallback Strategy)
        """
        if not self.api_ready:
            return "âš ï¸ API Key chÆ°a sáºµn sÃ ng."

        # âœ… CHIáº¾N THUáº¬T Æ¯U TIÃŠN: Pro -> Flash -> Exp
        if model_type == "pro":
            # Vá»›i task khÃ³ (Tranh biá»‡n): Æ¯u tiÃªn 3.0 Pro
            plan = [
                ("pro", "Gemini 2.5 pro", 6), 
                ("flash", "Gemini 2.5 Flash", 3), 
                ("exp", "Gemini 2.5 Flash exp", 3)
            ]
        else:
            # Vá»›i task thÆ°á»ng: Æ¯u tiÃªn Flash cho nhanh
            plan = [
                ("flash", "Gemini 2.5 Flash", 2), 
                ("exp", "Gemini 2.5 Flash exp", 2),
                ("pro", "Gemini 2.5 Pro", 6)
            ]

        last_errors = []
        quota_exhausted_count = 0

        for m_type, m_name, base_wait_time in plan:
            try:
                # Khá»Ÿi táº¡o model
                model = self._get_model(m_type, system_instr=system_instruction)
                if not model: continue
                
                # Gá»i API
                response = model.generate_content(prompt)
                
                # Kiá»ƒm tra káº¿t quáº£
                if response and hasattr(response, 'text') and response.text:
                    return response.text
                
                # Xá»­ lÃ½ cÃ¡c lÃ½ do bá»‹ cháº·n (Safety, Token...)
                if response and hasattr(response, 'candidates') and response.candidates:
                    candidate = response.candidates[0]
                    if hasattr(candidate, 'finish_reason'):
                        reason = candidate.finish_reason.name
                        if reason == "SAFETY":
                            last_errors.append(f"{m_name}: Bá»‹ cháº·n (Safety)")
                            continue
                        elif reason == "MAX_TOKENS":
                            last_errors.append(f"{m_name}: QuÃ¡ dÃ i (Max Tokens)")
                            continue
                
                last_errors.append(f"{m_name}: Tráº£ vá» rá»—ng")
                continue
            
            except ResourceExhausted:
                # Lá»—i háº¿t tiá»n/quota -> Chá» lÃ¢u hÆ¡n má»™t chÃºt rá»“i thá»­ model khÃ¡c
                quota_exhausted_count += 1
                error_msg = f"{m_name}: Háº¿t Quota (429)"
                last_errors.append(error_msg)
                time.sleep(base_wait_time * quota_exhausted_count)
                
            except (ServiceUnavailable, InternalServerError):
                # Lá»—i Server Google -> Chá» ngáº¯n
                last_errors.append(f"{m_name}: Lá»—i Server (5xx)")
                time.sleep(2)
            
            except InvalidArgument as e:
                # Lá»—i Input -> Dá»«ng luÃ´n, khÃ´ng thá»­ láº¡i
                return f"âš ï¸ Lá»—i Input (Prompt khÃ´ng há»£p lá»‡): {str(e)[:200]}"
                
            except Exception as e:
                last_errors.append(f"{m_name}: Lá»—i láº¡ ({str(e)[:50]})")
                time.sleep(1)

        # Náº¿u thá»­ háº¿t cÃ¡c model mÃ  váº«n lá»—i
        error_summary = "\n".join(f"- {e}" for e in last_errors[-3:])
        return f"âš ï¸ Há»‡ thá»‘ng Ä‘ang báº­n hoáº·c gáº·p lá»—i:\n{error_summary}\n\nğŸ’¡ Vui lÃ²ng thá»­ láº¡i sau 1 phÃºt."

    @staticmethod
    @st.cache_data(show_spinner=False, ttl=3600)
    def analyze_static(text, instruction):
        """
        HÃ m dÃ¹ng riÃªng cho RAG (Äá»c tÃ i liá»‡u) - CÃ³ Cache Ä‘á»ƒ tiáº¿t kiá»‡m tiá»n
        """
        try:
            api_key = st.secrets["api_keys"]["gemini_api_key"]
            genai.configure(api_key=api_key)
            
            # LuÃ´n dÃ¹ng Flash cho RAG vÃ¬ nÃ³ Ä‘á»c context dÃ i tá»‘t vÃ  ráº»
            model = genai.GenerativeModel(
                "gemini-2.5-flash",
                system_instruction=instruction
            )
            
            # Cáº¯t bá»›t náº¿u text quÃ¡ dÃ i (trÃ¡nh lá»—i quÃ¡ táº£i)
            max_chars = 200000 
            truncated_text = text[:max_chars]
            
            if len(text) > max_chars:
                st.warning(f"âš ï¸ TÃ i liá»‡u quÃ¡ dÃ i, chá»‰ phÃ¢n tÃ­ch {max_chars:,} kÃ½ tá»± Ä‘áº§u.")
            
            response = model.generate_content(truncated_text)
            
            if response and hasattr(response, 'text') and response.text:
                return response.text
            else:
                return "âš ï¸ KhÃ´ng cÃ³ pháº£n há»“i tá»« AI."
                
        except Exception as e:
            return f"âŒ Lá»—i phÃ¢n tÃ­ch tÄ©nh: {str(e)[:200]}"
